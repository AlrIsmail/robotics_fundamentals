{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Depth Estimation and Feature Tracking\n",
    "\n",
    "In the previous section, we discussed the concept of representation learning, which involves utilizing unsupervised and self-supervised methods to acquire an intermediate, low-dimensional representation of high-dimensional sensory data. These learned features can then be employed to solve various downstream visual inference tasks. In this section, we will delve into the application of representation learning in two common computer vision problems: monocular depth estimation and feature tracking.\n",
    "\n",
    "## Monocular Depth Estimation\n",
    "\n",
    "### Background\n",
    "\n",
    "Depth estimation is a fundamental building block in computer vision, essential for more complex tasks like 3D reconstruction, spatial perception in robotics, and navigation for autonomous vehicles. There are various methods for depth estimation, including structured light stereo and LIDAR (3D point clouds). However, in this discussion, we'll focus on passive depth estimation, as it doesn't require specialized and potentially expensive hardware and can perform well in outdoor scenarios.\n",
    "\n",
    "Depth estimation can be seen as a specific instance of the correspondence problem in computer vision. It involves identifying the 2D locations corresponding to the projections of a 3D point onto multiple 2D images captured from various viewpoints. The images can be acquired using either a monocular or stereo camera setup.\n",
    "\n",
    "One common method to solve the correspondence problem is through epipolar geometry, as illustrated in Figure 1. Given the camera centers O1 and O2 and a 3D point in the scene called P, p and p' represent the projection of P into the image planes for the left and right cameras, respectively. The epipolar constraint dictates that p' must lie somewhere on the epipolar line of the right camera when p is known, which is defined as the intersection of the image planes with the epipolar plane. This constraint is encapsulated by the fundamental (or essential) matrix between the two cameras, often denoted as F.\n",
    "\n",
    "![Figure 1: Epipolar geometry setup with a stereo camera.](images/epipolar_geometry.png)\n",
    "\n",
    "In the context of depth estimation, we usually assume a stereo setup with rectified images, where the epipolar lines are horizontal. The disparity, denoted as 'd,' represents the horizontal distance between corresponding points, and there is a simple inverse relationship between disparity and depth, specifically z = f * b / d, where f is the focal length of the cameras, and b is the length of the baseline between the two cameras. If we can find correspondences between rectified images, we can calculate their disparity and subsequently their depth.\n",
    "\n",
    "However, finding these correspondences is not a straightforward task, especially in real-world images with occlusions, repetitive patterns, and areas lacking texture. To address these challenges, modern representation learning methods are employed.\n",
    "\n",
    "### Supervised Estimation\n",
    "\n",
    "In this section, we focus on monocular depth estimation, where we have only a single image available at test time, and no assumptions about the scene contents are made. Fully supervised learning methods rely on training models, typically convolutional neural networks (CNNs), to predict pixel-wise disparity using pairs of ground truth depth and RGB camera frames. The training loss measures the similarity between the predicted and ground-truth depth, and the learning method aims to minimize this loss. Since monocular methods can only capture depth up to scale, a scale-invariant error is often used in addition to the traditional loss.\n",
    "\n",
    "![Figure 2: Vanilla supervised learning setup used in [1, 8, 11].](images/vanilla_learning.png)\n",
    "\n",
    "### Unsupervised Estimation\n",
    "\n",
    "While supervised learning methods have shown promising results, they are limited to scenes with abundant ground truth depth data. This limitation has led to the development of unsupervised learning methods, which leverage only the input RGB frame data and a stereo camera with known intrinsics. These methods eliminate the need for costly labeling efforts.\n",
    "\n",
    "One approach formulates the problem as image reconstruction using an autoencoder. The network learns to minimize the difference between the input reference image and a reconstructed version by using disparity maps as intermediate representations.\n",
    "\n",
    "![Figure 3: Unsupervised baseline network. The differentiable sampler enables end-to-end optimization.](images/unsupervised_baseline.png)\n",
    "The baseline network, shown in Figure 3, reconstructs the left image. It takes the left frame as input, maps it through a CNN to obtain disparity values, and then uses these disparities to reconstruct the left image. A fully differentiable bilinear sampler is used for end-to-end optimization.\n",
    "\n",
    "(Note: Continue the explanation for unsupervised estimation, supervised estimation, and self-supervised estimation for depth estimation, and then proceed to feature tracking, learned dense descriptors, and cross-object loss.)\n",
    "\n",
    "## Feature Tracking\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Feature tracking involves the task of tracking the locations of specific 2D points across a sequence of images. This is essential for understanding object motion in scenes captured by cameras. Similar to depth estimation, feature tracking also relies on solving the correspondence problem across image frames.\n",
    "\n",
    "![Figure 4: Feature point tracking over time.](images/feature_tracking.png)\n",
    "\n",
    "Feature tracking can be challenging due to the changing appearance of features across frames caused by factors such as camera movement, lighting changes, shadows, or occlusions. Ensuring consistent and reliable tracking of features is crucial.\n",
    "\n",
    "Traditionally, feature tracking has been tackled using hand-designed methods that identify and track distinctive features in images. These methods generate descriptors for these features to facilitate fast matching across frames.\n",
    "\n",
    "In this section, we explore how representation learning can be applied to learn descriptors for image features, eliminating the need for manual design.\n",
    "\n",
    "### Learned Dense Descriptors\n",
    "\n",
    "![Figure 5: Representation of dense descriptors.](images/dense_descriptors.png)\n",
    "\n",
    "The objective here is to learn a mapping that produces a dense D-dimensional descriptor for every pixel in an input color image. This means that we aim to obtain descriptors for every point in the image, not just a sparse set of distinctive features. This is achieved by training a neural network with a convolutional encoder-decoder architecture.\n",
    "\n",
    "The network is trained on pairs of images of the same object from different views, using a pixel-contrastive loss. This loss encourages similar descriptors for similar points and dissimilar descriptors for dissimilar points, facilitating feature matching.\n",
    "\n",
    "(Note: Continue the explanation for the contrastive loss, self-supervised data collection, cross-object loss, and class-consistent descriptors.)\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this section, we explored the application of representation learning in computer vision tasks, specifically monocular depth estimation and feature tracking. These techniques, which leverage unsupervised, supervised, and self-supervised learning, have the potential to enhance the performance of these tasks and reduce the reliance on manual feature design. As computer vision continues to advance, representation learning methods are likely to play an increasingly crucial role in addressing complex visual inference challenges.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
