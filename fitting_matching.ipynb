{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Methods in Computer Vision\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In computer vision, one of the fundamental tasks is fitting parametric models to observed data. This process involves finding the optimal parameters of a model that best describes the given data. In this notebook, we'll explore various fitting methods commonly used in computer vision, including least-squares fitting, the Random Sample Consensus (RANSAC) algorithm, and the Hough transform.\n",
    "\n",
    "## Least-Squares Fitting\n",
    "\n",
    "### Overview\n",
    "\n",
    "The goal of least-squares fitting is to find a parametric model that minimizes a chosen fitting error between the data and the model's estimate. One classic example is fitting a line to a set of (x, y) points. Other applications include computing 2D homographies or fundamental matrices.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a series of N 2D points { (xi, yi) } for i = 1 to N, least-squares fitting seeks to find a line of the form y = mx + b that minimizes the squared error in the y dimension. The objective is to find model parameters w = [m, b]ᵀ that minimize the sum of squared residuals between the observed data points (yi) and the model estimate (ŷi = mxᵢ + b).\n",
    "\n",
    "The residual, denoted as r, is defined as rᵢ = yi - ŷi. We aim to minimize the sum of squared residuals (E) using the least-squares method:\n",
    "\n",
    "E = Σ (rᵢ)² = Σ (yi - mxᵢ - b)²\n",
    "\n",
    "This can be represented in matrix notation as:\n",
    "\n",
    "E = ||Y - Xw||²\n",
    "\n",
    "where Y is a column vector of observed data points {yi}, X is a matrix containing the corresponding x values {xi, 1}, and w is the parameter vector [m, b]ᵀ.\n",
    "\n",
    "### Normal Equations\n",
    "\n",
    "To find the optimal parameters w, we set the gradient of the residual with respect to w to zero. This leads to the normal equations:\n",
    "\n",
    "XᵀXw = XᵀY\n",
    "\n",
    "The closed-form solution for w is given by:\n",
    "\n",
    "w = (XᵀX)⁻¹XᵀY\n",
    "\n",
    "However, it's important to note that this method fails when fitting vertical lines (m is undefined), leading to numerically unstable solutions. To address this, we can use an alternate line formulation in the form ax + by + d = 0.\n",
    "\n",
    "### Total Least Squares\n",
    "\n",
    "The total least squares method extends least squares fitting to account for errors in both the x and y axes. This approach minimizes the sum of squared orthogonal distances between data points and the line. The new set of parameters is represented as w = [a, b, c]ᵀ.\n",
    "\n",
    "### Robust Estimation\n",
    "\n",
    "Least-squares fitting is sensitive to outliers in the data. To mitigate this, robust cost functions can be used to penalize large residuals less. A common choice is the Huber loss, which transitions from quadratic to linear for large residuals.\n",
    "\n",
    "## Random Sample Consensus (RANSAC)\n",
    "\n",
    "### Overview\n",
    "\n",
    "The Random Sample Consensus (RANSAC) algorithm is designed to robustly fit models to data in the presence of outliers. It works by iteratively selecting random subsets of data points, fitting models to these subsets, and then evaluating the quality of each model against the entire data set.\n",
    "\n",
    "### RANSAC Procedure\n",
    "\n",
    "1. Randomly select a subset of data points (minimum number required to fit the model).\n",
    "2. Fit a model to the selected subset.\n",
    "3. Determine the inliers by comparing the fitted model to all data points (using a specified threshold).\n",
    "4. Repeat steps 1-3 for a fixed number of iterations.\n",
    "5. Select the model with the largest number of inliers as the best-fit model.\n",
    "\n",
    "RANSAC is useful when dealing with noisy data and outliers. It is important to tune parameters like the number of iterations and the threshold.\n",
    "\n",
    "## Hough Transform\n",
    "\n",
    "### Overview\n",
    "\n",
    "The Hough transform is another method for fitting models to data, particularly used for detecting lines or other shapes in images. It represents the data in a dual parameter space, which allows for the detection of lines without the need to calculate slopes.\n",
    "\n",
    "### Polar Parameterization\n",
    "\n",
    "The polar parameterization of lines is used in the Hough transform. Instead of using slope-intercept form, lines are represented as ρ = x * cos(θ) + y * sin(θ), where ρ is the perpendicular distance from the origin to the line, and θ is the angle between the x-axis and the line's normal vector.\n",
    "\n",
    "### Hough Voting\n",
    "\n",
    "In the Hough transform, data points vote for possible lines in parameter space. Each point in the image space corresponds to a sinusoidal profile in the Hough space. The intersection of these profiles identifies the fitted lines.\n",
    "\n",
    "### Advantages and Considerations\n",
    "\n",
    "The Hough transform is beneficial when dealing with unbounded slopes, but it requires a discretization of parameter space, introducing another parameter to tune. Larger grid cell sizes can handle noisy data, but may also merge different lines.\n",
    "\n",
    "In conclusion, these fitting methods are essential tools in computer vision for modeling and extracting information from data, each with its own advantages and considerations.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
