{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Understanding Dynamical Systems and Representation Learning\n",
    "\n",
    "## States and Representations\n",
    "\n",
    "The state of a dynamical system is a compressed description that captures the key aspects of the system, which can be time-varying or fixed parameters. For example, consider a walking robot in a vertical plane. Time-varying quantities may include information about the robot's current pose, joint configuration, and velocity, while fixed parameters could be limb lengths, limb weights, or joint friction. There are various possible representations for the system's state, such as joint angles, velocities, positions, or robot pose, and these can be represented differently, like Euler angles, angle-axis representation, or quaternions for orientations.\n",
    "\n",
    "The state of a dynamical system evolves over time, denoted as xt at time t. Many dynamical systems exhibit the Markov property, meaning future states depend only on the current state, not past states (Markov chains). This property simplifies predicting future states since they only depend on the present state. For example, if the state includes the current angular velocity of a joint, we can predict the joint angle at the next time step. This leads to modeling systems as Markov chains.\n",
    "\n",
    "![Markov Chain](images/markov_chain.png)\n",
    "\n",
    "In some cases, the state is not directly observable and must be estimated from noisy sensor observations, leading to hidden Markov models (HMMs). The observation at time t depends only on the current state, assuming conditional independence from past states and observations.\n",
    "\n",
    "![Hiden markov model](images/hmm.png)\n",
    "\n",
    "Robot actions or control inputs affecting state evolution will should be considered, leading to Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) for decision making.\n",
    "\n",
    "## Generative and Discriminative Approaches\n",
    "\n",
    "To estimate the state from observations, two high-level approaches are considered: generative and discriminative models. For example, estimating an object's pose (state x) from an RGB image (observation z) involves these approaches.\n",
    "\n",
    "1. **Generative Model**: Describes the joint probability distribution p(z, x) based on likelihood p(z|x) and prior p(x). Sampled poses (x(i)) from the prior can be used to generate likely observations (h(x(i)) = z(i)), and p(z|x(i)) provides the likelihood of the actual observation z given the hypothesized pose x(i).\n",
    "\n",
    "2. **Discriminative Model**: Models the conditional probability p(x|z) of the state x given the observation z directly. Neural networks, such as PoseCNN, can learn to map input images to likely output poses.\n",
    "\n",
    "![PoseCNN](images/PoseCNN.png)\n",
    "\n",
    "# Representation Learning\n",
    "\n",
    "## Representations in Computer Vision\n",
    "\n",
    "In computer vision, representations play a critical role. They exist in different forms at various stages of a high-level pipeline.\n",
    "![representation pipeline](images/representations_pipeline.png)\n",
    "\n",
    "- **Input Representation**: Describes the raw sensor data format, which can be images, depth data, or point clouds. It can vary, such as color vs. grayscale or different color spaces.\n",
    "\n",
    "- **Intermediate Representation**: A compressed, low-dimensional vector summarizing high-dimensional input sensory data, providing essential information for downstream tasks.\n",
    "\n",
    "- **Output Representation**: Describes key aspects of the scene necessary for decision-making, like object labels or 6D poses.\n",
    "\n",
    "These representations must meet several criteria, including being compact, explanatory, disentangled, hierarchical, and aiding downstream tasks.\n",
    "\n",
    "## Traditional CV and Interpretable Representations\n",
    "\n",
    "Traditional computer vision (pre-2012) used hand-crafted feature extraction methods for intermediate representations. These features, such as edges and textures, were designed for specific tasks, resulting in interpretable representations. However, designing these features was time-consuming and required domain expertise.\n",
    "![classic representation](images/classic_representation.png)\n",
    "\n",
    "## Modern CV and Learned Representations\n",
    "\n",
    "Modern computer vision (2012-present) uses learned intermediate representations, often based on Convolutional Neural Networks (CNNs). While these representations are powerful, they lack interpretability compared to traditional methods. Techniques like tSNE can project them into lower-dimensional spaces for better understanding.\n",
    "\n",
    "![learned intermediate representations](images/learned_representation.png)\n",
    "\n",
    "## Unsupervised and Self-Supervised Learning\n",
    "\n",
    "Unsupervised and self-supervised learning aim to learn meaningful representations without explicit labels.\n",
    "\n",
    "- **Unsupervised Learning**: Autoencoders, like the one in the figure bellow, seek to reconstruct input data. The compressed intermediate representation (bottleneck) becomes meaningful for downstream tasks.\n",
    "\n",
    "![autoencoders reconstruct input data](images/autoencoders.png)\n",
    "\n",
    "- **Self-Supervised Learning**: Training on partially masked input data, the network learns representations by predicting masked parts, enabling downstream classification with minimal labeled data.\n",
    "![autoencoders partially masked input](images/predecting_image_completion.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
