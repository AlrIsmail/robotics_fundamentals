{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration and 3D Reconstruction\n",
    "\n",
    "## Camera Calibration\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this section,\n",
    "\n",
    "* We will learn about distortions in camera, intrinsic and extrinsic parameters of camera etc.\n",
    "\n",
    "### Basics\n",
    "\n",
    "#### What is camera calibration?\n",
    "\n",
    "For applications like object detection, we need to know what object is where. For that, we need to know camera properties and its parameters. For example, what are all the distortions in that camera? What is focal length of camera? What is optical center of camera lens? These information is intrinsic to that camera. We also need to know the position of camera in world co-ordinate system. These information is extrinsic to camera. So in short, we need to find the relation between points of 3D world and 2D image. Finding these intrinsic and extrinsic parameters of camera is called **camera calibration**.\n",
    "\n",
    "#### Camera Types\n",
    "\n",
    "There are various types of camera models available like pinhole camera model, fisheye camera model etc. Most popular one is pinhole camera model. It is simple. Once we get intrinsic parameters of camera, we can use it to correct the distortions in real time. \n",
    "\n",
    "#### Distortions\n",
    "\n",
    "There are two types of distortions:\n",
    "\n",
    "* Radial Distortion\n",
    "* Tangential Distortion\n",
    "\n",
    "##### Radial Distortion\n",
    "\n",
    "Straight lines will appear curved in images. It is caused by the unequal scaling of axes. So once we know camera parameters, we can correct this distortion.\n",
    "\n",
    "##### Tangential Distortion\n",
    "\n",
    "It occurs because image taking lense is not aligned perfectly parallel to the imaging plane. So some areas in image may look nearer than expected.\n",
    "\n",
    "#### Intrinsic Parameters\n",
    "\n",
    "Intrinsic parameters are specific to a camera. It includes information like focal length $(f_x, f_y)$, optical centers $(c_x, c_y)$ etc. It is also called camera matrix. It depends on the camera only, so once calculated, it can be stored for future purposes. It is expressed as a 3x3 matrix:\n",
    "\n",
    "$$\n",
    "cameraMatrix = \\begin{bmatrix}\n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Extrinsic Parameters\n",
    "\n",
    "Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system.\n",
    "\n",
    "So with the help of these parameters, we can find the 3D world coordinates of any point in that image if we know its coordinates in that image and vice-versa.\n",
    "\n",
    "#### So what we do is\n",
    "\n",
    "* We take 10-20 images of a chess board.\n",
    "* We find some specific points in it ( square corners in chess board).\n",
    "* We know its coordinates in real world space and we know its coordinates in image.\n",
    "* With these data, some mathematical problem is solved in background to get the distortion coefficients, camera matrix etc. which are the intrinsic parameters of that camera.\n",
    "* Now we can take an image, undistort it, use cv2.remap() to remap the pixels to canonical image and get a image without distortion.\n",
    "\n",
    "### Code\n",
    "\n",
    "We need at least 10 test patterns for camera calibration. We are using chess board as test pattern. The important input data needed for calibration of the camera is the set of 3D real world points and the corresponding 2D coordinates of these points in the image. 2D images points are OK which we can easily find from the image. (These image points are locations where two black squares touch each other in chess boards).  \n",
    "\n",
    "What about the 3D points from real world space? Those images are taken from a static camera and chess boards are placed at different locations and orientations. So we need to know (X,Y,Z) values. But for simplicity, we can say chess board was kept stationary at XY plane, (so Z=0 always) and camera was moved accordingly. This consideration helps us to find only X,Y values. Now for X,Y values, we can simply pass the points as (0,0), (1,0), (2,0), ... which denotes the location of points. In this case, the results we get will be in the scale of size of chess board square. But if we know the square size, (say 30 mm), and we can pass the values as (0,0),(30,0),(60,0),..., we get the results in mm. (In this case, we don't know square size since we didn't take those images, so we pass in terms of square size).\n",
    "\n",
    "3D points are called object points and 2D image points are called image points.\n",
    "\n",
    "#### Setup\n",
    "\n",
    "So to find pattern in chess board, we can use the function, cv2.findChessboardCorners(). We also need to pass what kind of pattern we are looking, like 8x8 grid, 5x5 grid etc. In this example, we use 7x6 grid. (Normally a chess board has 8x8 squares and 7x7 internal corners). It returns the corner points and retval which will be True if pattern is obtained. These corners will be placed in an order (from left-to-right, top-to-bottom)\n",
    "\n",
    "##### Note\n",
    "This function cannot find the chess board corners in all the images. So one good option is to write our own code so it starts the camera and check each frame for required pattern. If found, add to a list of found corners. Even if a failure happens in middle, we can take next image and continue the process. \n",
    "\n",
    "Instead of chess board, we can use some circular grid, but then use cv2.findCirclesGrid() to find the pattern. It is said that less number of images are enough when using circular grid.\n",
    "\n",
    "Once we find the corners, we can increase their accuracy using cv2.cornerSubPix(). We can also draw the pattern using cv2.drawChessboardCorners()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "# to get the file pathnames from opencv github\n",
    "import glob\n",
    "\n",
    "# termination criteria\n",
    "# Criteria for termination of the iterative process of corner refinement.\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "# 3d point in real world space\n",
    "objpoints = []\n",
    "# 2d points in image plane.\n",
    "imgpoints = []\n",
    "# Download the images from opencv github if you don't have them\n",
    "# Get all the file pathnames\n",
    "images = glob.glob('data/left*.jpg')\n",
    "\n",
    "\n",
    "for fname in images:\n",
    "    # Read the image\n",
    "    img = cv.imread(fname)\n",
    "    # Convert to grayscale\n",
    "    gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (7,6),None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        # Add the object points\n",
    "        objpoints.append(objp)\n",
    "        # Refine the image points\n",
    "        corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "        # Add the image points\n",
    "        imgpoints.append(corners2)\n",
    "        # Draw and display the corners\n",
    "        img = cv.drawChessboardCorners(img, (7,6), corners2,ret)\n",
    "        cv.imshow('img',img)\n",
    "        cv.waitKey(500)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration\n",
    "\n",
    "So now we have our object points and image points we are ready to go for calibration. For that we use the function, cv2.calibrateCamera(). It returns the camera matrix, distortion coefficients, rotation and translation vectors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alrismail/Documents/robotics_fundamentals/opencv_camera_calibration_3d_reconstruction.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alrismail/Documents/robotics_fundamentals/opencv_camera_calibration_3d_reconstruction.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ret, mtx, dist, rvecs, tvecs \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mcalibrateCamera(objpoints, imgpoints, gray\u001b[39m.\u001b[39mshape[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mNone\u001b[39;00m,\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gray' is not defined"
     ]
    }
   ],
   "source": [
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undistortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
